{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Nurse hate thread\" - The pandemic's female Other "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrape_resultat import nurse_text\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(nurse_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NURSE HEROS</td>\n",
       "      <td>[\\n                    Did everyone just sudde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>* QUESTION * Do you agree with this nurse, to ...</td>\n",
       "      <td>[\\n                    /pol/ : do you agree wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>White American Nurse Kills 89 patients in Hosp...</td>\n",
       "      <td>[\\nhttps://www.bbc.com/news/world-europe-48539...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                        NURSE HEROS   \n",
       "1  * QUESTION * Do you agree with this nurse, to ...   \n",
       "2  White American Nurse Kills 89 patients in Hosp...   \n",
       "\n",
       "                                            comments  \n",
       "0  [\\n                    Did everyone just sudde...  \n",
       "1  [\\n                    /pol/ : do you agree wi...  \n",
       "2  [\\nhttps://www.bbc.com/news/world-europe-48539...  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleancomment(x):\n",
    "    l=x[0].strip()\n",
    "    a1=re.sub(r\"\\n \",\" \",l)\n",
    "    a2=re.sub(r\"\\n\",\" \",a1)\n",
    "    a3=re.sub('(>{0,2}\\d{9}>{0,2})', ' ', a2)\n",
    "    return(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean\"]=df[\"comments\"].apply(cleancomment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NURSE HEROS</td>\n",
       "      <td>[\\n                    Did everyone just sudde...</td>\n",
       "      <td>Did everyone just suddenly forget there Nurse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>* QUESTION * Do you agree with this nurse, to ...</td>\n",
       "      <td>[\\n                    /pol/ : do you agree wi...</td>\n",
       "      <td>/pol/ : do you agree with this Nurse ?? should...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>White American Nurse Kills 89 patients in Hosp...</td>\n",
       "      <td>[\\nhttps://www.bbc.com/news/world-europe-48539...</td>\n",
       "      <td>https://www.bbc.com/news/world-europe-48539894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                        NURSE HEROS   \n",
       "1  * QUESTION * Do you agree with this nurse, to ...   \n",
       "2  White American Nurse Kills 89 patients in Hosp...   \n",
       "\n",
       "                                            comments  \\\n",
       "0  [\\n                    Did everyone just sudde...   \n",
       "1  [\\n                    /pol/ : do you agree wi...   \n",
       "2  [\\nhttps://www.bbc.com/news/world-europe-48539...   \n",
       "\n",
       "                                               clean  \n",
       "0  Did everyone just suddenly forget there Nurse ...  \n",
       "1  /pol/ : do you agree with this Nurse ?? should...  \n",
       "2     https://www.bbc.com/news/world-europe-48539894  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ok /pol/, let's talk nurses.  This is a collection of women and faggots who weren't smart enough to become doctors and now suddenly are national heroes. A piece of shit out of my asshole could become a nurse.  fucking retards\""
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nurse_sample = df['clean'].iloc[23]\n",
    "nurse_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(nurse_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok INTJ intj ok\n",
      "/pol/ PUNCT punct /pol/\n",
      ", PUNCT punct ,\n",
      "let VERB ROOT let\n",
      "'s PRON nsubj -PRON-\n",
      "talk VERB ccomp talk\n",
      "nurses NOUN dobj nurse\n",
      ". PUNCT punct .\n",
      "  SPACE   \n",
      "This DET nsubj this\n",
      "is AUX ROOT be\n",
      "a DET det a\n",
      "collection NOUN attr collection\n",
      "of ADP prep of\n",
      "women NOUN pobj woman\n",
      "and CCONJ cc and\n",
      "faggots NOUN conj faggot\n",
      "who PRON nsubj who\n",
      "were AUX relcl be\n",
      "n't PART neg not\n",
      "smart ADJ acomp smart\n",
      "enough ADV advmod enough\n",
      "to PART aux to\n",
      "become VERB xcomp become\n",
      "doctors NOUN attr doctor\n",
      "and CCONJ cc and\n",
      "now ADV advmod now\n",
      "suddenly ADV advmod suddenly\n",
      "are AUX conj be\n",
      "national ADJ amod national\n",
      "heroes NOUN attr hero\n",
      ". PUNCT punct .\n",
      "A DET det a\n",
      "piece NOUN nsubj piece\n",
      "of ADP prep of\n",
      "shit NOUN pobj shit\n",
      "out SCONJ prep out\n",
      "of ADP prep of\n",
      "my DET poss -PRON-\n",
      "asshole NOUN pobj asshole\n",
      "could VERB aux could\n",
      "become VERB ROOT become\n",
      "a DET det a\n",
      "nurse NOUN attr nurse\n",
      ". PUNCT punct .\n",
      "  SPACE   \n",
      "fucking PROPN ROOT fucking\n",
      "retards NOUN dobj retard\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9880432912405939014, 728, 729), (9880432912405939014, 1404, 1405), (9880432912405939014, 1565, 1566), (9880432912405939014, 2308, 2309), (9880432912405939014, 2521, 2522), (9880432912405939014, 4587, 4588), (9880432912405939014, 6222, 6223), (9880432912405939014, 9101, 9102), (9880432912405939014, 10613, 10614), (9880432912405939014, 12211, 12212), (9880432912405939014, 12277, 12278), (9880432912405939014, 12858, 12859), (9880432912405939014, 13079, 13080), (9880432912405939014, 13142, 13143), (9880432912405939014, 14553, 14554), (9880432912405939014, 14881, 14882), (9880432912405939014, 19337, 19338), (9880432912405939014, 20577, 20578), (9880432912405939014, 21669, 21670), (9880432912405939014, 24269, 24270), (9880432912405939014, 32513, 32514), (9880432912405939014, 33815, 33816), (9880432912405939014, 37452, 37453), (9880432912405939014, 40969, 40970), (9880432912405939014, 41520, 41521), (9880432912405939014, 44900, 44901), (9880432912405939014, 45169, 45170), (9880432912405939014, 49781, 49782), (9880432912405939014, 53965, 53966), (9880432912405939014, 54131, 54132), (9880432912405939014, 54235, 54236), (9880432912405939014, 54492, 54493), (9880432912405939014, 54842, 54843), (9880432912405939014, 54995, 54996), (9880432912405939014, 55131, 55132), (9880432912405939014, 59070, 59071), (9880432912405939014, 60534, 60535), (9880432912405939014, 62554, 62555), (9880432912405939014, 63128, 63129), (9880432912405939014, 63384, 63385), (9880432912405939014, 64092, 64093), (9880432912405939014, 64112, 64113), (9880432912405939014, 64121, 64122), (9880432912405939014, 64442, 64443), (9880432912405939014, 65098, 65099), (9880432912405939014, 65105, 65106), (9880432912405939014, 66696, 66697), (9880432912405939014, 66725, 66726), (9880432912405939014, 66824, 66825), (9880432912405939014, 66936, 66937), (9880432912405939014, 66968, 66969), (9880432912405939014, 67008, 67009), (9880432912405939014, 67621, 67622), (9880432912405939014, 69728, 69729), (9880432912405939014, 70281, 70282), (9880432912405939014, 70762, 70763), (9880432912405939014, 72408, 72409), (9880432912405939014, 72430, 72431), (9880432912405939014, 72475, 72476), (9880432912405939014, 72519, 72520), (9880432912405939014, 73441, 73442), (9880432912405939014, 73447, 73448), (9880432912405939014, 73483, 73484), (9880432912405939014, 73489, 73490), (9880432912405939014, 73975, 73976), (9880432912405939014, 73997, 73998), (9880432912405939014, 74793, 74794), (9880432912405939014, 77449, 77450), (9880432912405939014, 82977, 82978), (9880432912405939014, 87773, 87774), (9880432912405939014, 90806, 90807), (9880432912405939014, 91328, 91329), (9880432912405939014, 92856, 92857), (9880432912405939014, 93247, 93248), (9880432912405939014, 95749, 95750), (9880432912405939014, 97629, 97630), (9880432912405939014, 105379, 105380), (9880432912405939014, 107298, 107299), (9880432912405939014, 107557, 107558), (9880432912405939014, 107642, 107643), (9880432912405939014, 108104, 108105), (9880432912405939014, 110120, 110121)]\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "phrase_list = ['bitch', 'hero']\n",
    "phrase_patterns = [nlp(text) for text in phrase_list]\n",
    "matcher.add(\"nurse_pattern\", None, *phrase_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches = matcher(doc3)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# I used the first half of the scrape because the Matcher only processes 1000000 strings and nurse_text has 1444217.\n",
    "length = len(nurse_text)\n",
    "middle_index = length/2\n",
    "first_half = nurse_text[:middle_index]\n",
    "second_half = nurse_text[middle_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(str(first_half))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(str(second_half))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9880432912405939014, 813, 814), (9880432912405939014, 1812, 1813), (9880432912405939014, 3403, 3404), (9880432912405939014, 4073, 4074), (9880432912405939014, 4300, 4301), (9880432912405939014, 5056, 5057), (9880432912405939014, 5077, 5078), (9880432912405939014, 12570, 12571), (9880432912405939014, 21436, 21437), (9880432912405939014, 21619, 21620), (9880432912405939014, 24145, 24146), (9880432912405939014, 35454, 35455), (9880432912405939014, 36163, 36164), (9880432912405939014, 37426, 37427), (9880432912405939014, 38558, 38559), (9880432912405939014, 39471, 39472), (9880432912405939014, 40187, 40188), (9880432912405939014, 41986, 41987), (9880432912405939014, 43000, 43001), (9880432912405939014, 43238, 43239), (9880432912405939014, 43298, 43299), (9880432912405939014, 43313, 43314), (9880432912405939014, 45469, 45470), (9880432912405939014, 45497, 45498), (9880432912405939014, 45522, 45523), (9880432912405939014, 46187, 46188), (9880432912405939014, 46446, 46447), (9880432912405939014, 47970, 47971), (9880432912405939014, 48074, 48075), (9880432912405939014, 54781, 54782), (9880432912405939014, 57264, 57265), (9880432912405939014, 62991, 62992), (9880432912405939014, 84071, 84072), (9880432912405939014, 86970, 86971), (9880432912405939014, 87320, 87321), (9880432912405939014, 88154, 88155), (9880432912405939014, 88865, 88866), (9880432912405939014, 91578, 91579), (9880432912405939014, 91602, 91603), (9880432912405939014, 96963, 96964), (9880432912405939014, 100022, 100023), (9880432912405939014, 100590, 100591), (9880432912405939014, 105291, 105292), (9880432912405939014, 105522, 105523), (9880432912405939014, 108679, 108680), (9880432912405939014, 109820, 109821), (9880432912405939014, 110639, 110640), (9880432912405939014, 111632, 111633), (9880432912405939014, 112156, 112157), (9880432912405939014, 116881, 116882), (9880432912405939014, 117457, 117458), (9880432912405939014, 118898, 118899), (9880432912405939014, 119279, 119280), (9880432912405939014, 120413, 120414), (9880432912405939014, 121610, 121611), (9880432912405939014, 123507, 123508), (9880432912405939014, 125615, 125616), (9880432912405939014, 130008, 130009), (9880432912405939014, 130118, 130119), (9880432912405939014, 130301, 130302), (9880432912405939014, 134421, 134422), (9880432912405939014, 136183, 136184), (9880432912405939014, 140977, 140978), (9880432912405939014, 142587, 142588), (9880432912405939014, 143201, 143202), (9880432912405939014, 146648, 146649), (9880432912405939014, 148125, 148126), (9880432912405939014, 158824, 158825), (9880432912405939014, 166947, 166948), (9880432912405939014, 171374, 171375), (9880432912405939014, 171391, 171392), (9880432912405939014, 185073, 185074)]\n"
     ]
    }
   ],
   "source": [
    "# second half of the scrape\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "phrase_list = ['bitch', 'hero']\n",
    "phrase_patterns = [nlp(text) for text in phrase_list]\n",
    "matcher.add(\"nurse_pattern\", None, *phrase_patterns)\n",
    "found_matches = matcher(doc4)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matches, first half of scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LEMMA\":\"hero\"}]\n",
    "matcher.add(\"nurse_pattern\", None, pattern)\n",
    "matches = matcher(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches = matcher(doc3)\n",
    "# found_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [sent for sent in doc3.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n\\n2-3 weeks ago all I saw was FB posts about our essential worker \"heros\".\n"
     ]
    }
   ],
   "source": [
    "# Show the 1st sentence, which contains a match\n",
    "for exempli in sents:\n",
    "    if found_matches[0][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* heroes!\n"
     ]
    }
   ],
   "source": [
    "# Show the 2nd sentence, which contains a match\n",
    "for exempli in sents:\n",
    "    if found_matches[1][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a hero. ', ' ', ' >>257906840i know a nurse that doesnt know her own blood type or her childrens blood types ', ' >>\n"
     ]
    }
   ],
   "source": [
    "# Show the 3rd sentence, which contains a match\n",
    "for exempli in sents:\n",
    "    if found_matches[2][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It makes it worse that these young American women are supposedly heroâ€™s and have a god complex.\n"
     ]
    }
   ],
   "source": [
    "for exempli in sents:\n",
    "    if found_matches[3][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are the unsung heroes in all of this and deserve applause. ', ' >>\n"
     ]
    }
   ],
   "source": [
    "for exempli in sents:\n",
    "    if found_matches[97][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break\n",
    "# 97 seems to be the last \"hero\" in first half of scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matches, second half of scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LEMMA\":\"hero\"}]\n",
    "matcher.add(\"nurse_pattern\", None, pattern)\n",
    "matches = matcher(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches2 = matcher(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents2 = [sent for sent in doc4.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A lot of people are wondering if they'll be able to feed their kids in the next few months because of over-reacting medical 'professionals' in the media blowing things out of proportion. \", ' If a few months of actual hard work makes you feel like \"a hero without a cape\" or \"an underappreciated protector\" then your profession isn\\'t used to hard workMost thots became nurses for social validation and so that they could appear\n"
     ]
    }
   ],
   "source": [
    "for exempli in sents2:\n",
    "    if found_matches2[0][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The real heroes ', ' >>\n"
     ]
    }
   ],
   "source": [
    "for exempli in sents2:\n",
    "    if found_matches2[5][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the military is going to slowly take a backseat now and be replaced with a new hero: nurses and doctors.\n"
     ]
    }
   ],
   "source": [
    "for exempli in sents2:\n",
    "    if found_matches2[21][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break\n",
    "# 21 seems to be last \"hero\" in second half of scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LEMMA\":\"bitch\"}]\n",
    "matcher.add(\"nurse_pattern\", None, pattern)\n",
    "matches = matcher(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches3 = matcher(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents3 = [sent for sent in doc4.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesus Christ you dumb bitch.\n"
     ]
    }
   ],
   "source": [
    "for exempli in sents3:\n",
    "    if found_matches3[3][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c7f35072e150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexempli\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mfound_matches3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m83\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mexempli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexempli\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 83 seems to be the last \"bitch\" in second half of scrape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for exempli in sents3:\n",
    "    if found_matches3[83][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break\n",
    "# 83 seems to be the last \"bitch\" in second half of scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
