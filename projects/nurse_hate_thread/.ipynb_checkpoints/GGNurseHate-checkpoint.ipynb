{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Nurse hate thread\" - The pandemic's female Other "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrape_resultat import nurse_text\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(nurse_text) \n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NURSE HEROS</td>\n",
       "      <td>[\\n                    Did everyone just sudde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>* QUESTION * Do you agree with this nurse, to ...</td>\n",
       "      <td>[\\n                    /pol/ : do you agree wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>White American Nurse Kills 89 patients in Hosp...</td>\n",
       "      <td>[\\nhttps://www.bbc.com/news/world-europe-48539...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                        NURSE HEROS   \n",
       "1  * QUESTION * Do you agree with this nurse, to ...   \n",
       "2  White American Nurse Kills 89 patients in Hosp...   \n",
       "\n",
       "                                            comments  \n",
       "0  [\\n                    Did everyone just sudde...  \n",
       "1  [\\n                    /pol/ : do you agree wi...  \n",
       "2  [\\nhttps://www.bbc.com/news/world-europe-48539...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleancomment(x):\n",
    "    l=x[0].strip()\n",
    "    a1=re.sub(r\"\\n \",\" \",l)\n",
    "    a2=re.sub(r\"\\n\",\" \",a1)\n",
    "    a3=re.sub('(>{0,2}\\d{9}>{0,2})', ' ', a2)\n",
    "    return(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean\"]=df[\"comments\"].apply(cleancomment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NURSE HEROS</td>\n",
       "      <td>[\\n                    Did everyone just sudde...</td>\n",
       "      <td>Did everyone just suddenly forget there Nurse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>* QUESTION * Do you agree with this nurse, to ...</td>\n",
       "      <td>[\\n                    /pol/ : do you agree wi...</td>\n",
       "      <td>/pol/ : do you agree with this Nurse ?? should...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>White American Nurse Kills 89 patients in Hosp...</td>\n",
       "      <td>[\\nhttps://www.bbc.com/news/world-europe-48539...</td>\n",
       "      <td>https://www.bbc.com/news/world-europe-48539894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                        NURSE HEROS   \n",
       "1  * QUESTION * Do you agree with this nurse, to ...   \n",
       "2  White American Nurse Kills 89 patients in Hosp...   \n",
       "\n",
       "                                            comments  \\\n",
       "0  [\\n                    Did everyone just sudde...   \n",
       "1  [\\n                    /pol/ : do you agree wi...   \n",
       "2  [\\nhttps://www.bbc.com/news/world-europe-48539...   \n",
       "\n",
       "                                               clean  \n",
       "0  Did everyone just suddenly forget there Nurse ...  \n",
       "1  /pol/ : do you agree with this Nurse ?? should...  \n",
       "2     https://www.bbc.com/news/world-europe-48539894  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ok /pol/, let's talk nurses.  This is a collection of women and faggots who weren't smart enough to become doctors and now suddenly are national heroes. A piece of shit out of my asshole could become a nurse.  fucking retards\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nurse_sample = df['clean'].iloc[23]\n",
    "nurse_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(nurse_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok INTJ intj ok\n",
      "/pol/ PUNCT punct /pol/\n",
      ", PUNCT punct ,\n",
      "let VERB ROOT let\n",
      "'s PRON nsubj -PRON-\n",
      "talk VERB ccomp talk\n",
      "nurses NOUN dobj nurse\n",
      ". PUNCT punct .\n",
      "  SPACE   \n",
      "This DET nsubj this\n",
      "is AUX ROOT be\n",
      "a DET det a\n",
      "collection NOUN attr collection\n",
      "of ADP prep of\n",
      "women NOUN pobj woman\n",
      "and CCONJ cc and\n",
      "faggots NOUN conj faggot\n",
      "who PRON nsubj who\n",
      "were AUX relcl be\n",
      "n't PART neg not\n",
      "smart ADJ acomp smart\n",
      "enough ADV advmod enough\n",
      "to PART aux to\n",
      "become VERB xcomp become\n",
      "doctors NOUN attr doctor\n",
      "and CCONJ cc and\n",
      "now ADV advmod now\n",
      "suddenly ADV advmod suddenly\n",
      "are AUX conj be\n",
      "national ADJ amod national\n",
      "heroes NOUN attr hero\n",
      ". PUNCT punct .\n",
      "A DET det a\n",
      "piece NOUN nsubj piece\n",
      "of ADP prep of\n",
      "shit NOUN pobj shit\n",
      "out SCONJ prep out\n",
      "of ADP prep of\n",
      "my DET poss -PRON-\n",
      "asshole NOUN pobj asshole\n",
      "could VERB aux could\n",
      "become VERB ROOT become\n",
      "a DET det a\n",
      "nurse NOUN attr nurse\n",
      ". PUNCT punct .\n",
      "  SPACE   \n",
      "fucking PROPN ROOT fucking\n",
      "retards NOUN dobj retard\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"doc\"]=df[\"clean\"].apply(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slice parts for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slices parts for speed \n",
    "dfsample=df[68:110]\n",
    "len(dfsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: random sample\n",
    "dfsample=df.sample(frac=0.1)\n",
    "len(dfsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: random sample\n",
    "dfsample=df.sample(n=10)\n",
    "len(dfsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch for the slice for the follow up speeding up\n",
    "df=dfsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                         staged photo of nurse and truck\n",
       "comments    [\\n                    can someone repost the ...\n",
       "clean       can someone repost the real photos of the stag...\n",
       "doc         (can, someone, repost, the, real, photos, of, ...\n",
       "Name: 40, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one example\n",
    "dfexem=df.iloc[4]\n",
    "dfexem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "can someone repost the real photos of the staged nurse atanding in front of the truck with the woman in the USA shirt? need to refute someone"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfexem[\"doc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic preprocessing\n",
    "\n",
    "https://realpython.com/natural-language-processing-spacy-python/\n",
    "\n",
    "## Variations of semantic analysis\n",
    "- sentences\n",
    "- noun chunks\n",
    "- NER (named entity recognition)\n",
    "- lemmatisation\n",
    "- stop word removal\n",
    "- pattern matcher\n",
    "- similar expressions, dictionary, thesaurus\n",
    "- semantic clusters\n",
    "- semantic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* für Post:\n",
      " ok /pol/, let's talk nurses.  This is a collection of women and faggots who weren't smart enough to become doctors and now suddenly are national heroes. A piece of shit out of my asshole could become a nurse.  fucking retards\n",
      "******\n",
      "mit Satz 0: ok /pol/, let's talk nurses.  \n",
      "mit Satz 1: This is a collection of women and faggots who weren't smart enough to become doctors and now suddenly are national heroes.\n",
      "mit Satz 2: A piece of shit out of my asshole could become a nurse.  \n",
      "mit Satz 3: fucking retards\n"
     ]
    }
   ],
   "source": [
    "print(f\"******* für Post:\\n {doc}\\n******\")\n",
    "for i,sent in enumerate(doc.sents):\n",
    "    print(f\"mit Satz {i}: {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "someone\n",
      "the real photos\n",
      "the staged nurse\n",
      "front\n",
      "the truck\n",
      "the woman\n",
      "the USA shirt\n",
      "someone\n"
     ]
    }
   ],
   "source": [
    "for np in dfexem[\"doc\"].noun_chunks:\n",
    "    print(np.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "phrase_list = ['bitch', 'hero']\n",
    "phrase_patterns = [nlp(text) for text in phrase_list]\n",
    "matcher.add(\"nurse_pattern\", None, *phrase_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_match(doc):\n",
    "    m=matcher(doc)\n",
    "    return(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research objects\n",
    "\n",
    "Language of attributions of hate subjects (nuns) in context of special discourse\n",
    "\n",
    "- Expressions:\n",
    "    - Sentences\n",
    "    - noun expressions\n",
    "    - attributes of nuns\n",
    "- subjects\n",
    "    - hate subjects\n",
    "        - nurses\n",
    "- attributions\n",
    "    - attributes\n",
    "    - curses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataframe of research objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baustelle of unfinished fragments‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "found_matches = matcher(doc3)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# I used the first half of the scrape because the Matcher only processes 1000000 strings and nurse_text has 1444217.\n",
    "length = len(nurse_text)\n",
    "middle_index = length/2\n",
    "first_half = nurse_text[:middle_index]\n",
    "second_half = nurse_text[middle_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'first_half' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c440bebac9a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_half\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'first_half' is not defined"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(str(first_half))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(str(second_half))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second half of the scrape\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "phrase_list = ['bitch', 'hero']\n",
    "phrase_patterns = [nlp(text) for text in phrase_list]\n",
    "matcher.add(\"nurse_pattern\", None, *phrase_patterns)\n",
    "found_matches = matcher(doc4)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matches, first half of scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LEMMA\":\"hero\"}]\n",
    "matcher.add(\"nurse_pattern\", None, pattern)\n",
    "matches = matcher(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches = matcher(doc3)\n",
    "# found_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [sent for sent in doc3.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the 1st sentence, which contains a match\n",
    "for exempli in sents:\n",
    "    if found_matches[0][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the 2nd sentence, which contains a match\n",
    "for exempli in sents:\n",
    "    if found_matches[1][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the 3rd sentence, which contains a match\n",
    "for exempli in sents:\n",
    "    if found_matches[2][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exempli in sents:\n",
    "    if found_matches[3][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exempli in sents:\n",
    "    if found_matches[97][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break\n",
    "# 97 seems to be the last \"hero\" in first half of scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matches, second half of scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LEMMA\":\"hero\"}]\n",
    "matcher.add(\"nurse_pattern\", None, pattern)\n",
    "matches = matcher(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches2 = matcher(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents2 = [sent for sent in doc4.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exempli in sents2:\n",
    "    if found_matches2[0][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exempli in sents2:\n",
    "    if found_matches2[5][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exempli in sents2:\n",
    "    if found_matches2[21][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break\n",
    "# 21 seems to be last \"hero\" in second half of scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LEMMA\":\"bitch\"}]\n",
    "matcher.add(\"nurse_pattern\", None, pattern)\n",
    "matches = matcher(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches3 = matcher(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents3 = [sent for sent in doc4.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exempli in sents3:\n",
    "    if found_matches3[3][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exempli in sents3:\n",
    "    if found_matches3[83][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break\n",
    "# 83 seems to be the last \"bitch\" in second half of scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
