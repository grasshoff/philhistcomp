{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Nurse hate thread\" - The pandemic's female Other "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrape_resultat import nurse_text\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(nurse_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe from list\n",
    "lst_col = 'comments'\n",
    "\n",
    "alle = pd.DataFrame({\n",
    "      col:np.repeat(df[col].values, df[lst_col].str.len())\n",
    "      for col in df.columns.drop(lst_col)}\n",
    "    ).assign(**{lst_col:np.concatenate(df[lst_col].values)})[df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alle['comments'] = [re.sub('(>{0,2}\\d{9}>{0,2})', ' ', i) for i in alle.comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alle['comments'] = [i.replace('∖n ', ' ').replace('∖n', ' ') for i in alle.comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NURSE HEROS</td>\n",
       "      <td>\\n                    Did everyone just sudden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>* QUESTION * Do you agree with this nurse, to ...</td>\n",
       "      <td>\\n                    /pol/ : do you agree wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>* QUESTION * Do you agree with this nurse, to ...</td>\n",
       "      <td>why does she dislike the white areas of Georg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>* QUESTION * Do you agree with this nurse, to ...</td>\n",
       "      <td>As a Nurse, shouldn't she be trying to provid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>* QUESTION * Do you agree with this nurse, to ...</td>\n",
       "      <td>Summa Cum Laude and she still can't speak pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>* QUESTION * Do you agree with this nurse, to ...</td>\n",
       "      <td>makes ya think why they don't and only do it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>* QUESTION * Do you agree with this nurse, to ...</td>\n",
       "      <td>Shes a nigger psycho, she has a mindless imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>* QUESTION * Do you agree with this nurse, to ...</td>\n",
       "      <td>It can’t even form a complete sentence. It c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>* QUESTION * Do you agree with this nurse, to ...</td>\n",
       "      <td>it's latin, retard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>* QUESTION * Do you agree with this nurse, to ...</td>\n",
       "      <td>lol hate to brake it to ya honey but sav and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                        NURSE HEROS   \n",
       "1  * QUESTION * Do you agree with this nurse, to ...   \n",
       "2  * QUESTION * Do you agree with this nurse, to ...   \n",
       "3  * QUESTION * Do you agree with this nurse, to ...   \n",
       "4  * QUESTION * Do you agree with this nurse, to ...   \n",
       "5  * QUESTION * Do you agree with this nurse, to ...   \n",
       "6  * QUESTION * Do you agree with this nurse, to ...   \n",
       "7  * QUESTION * Do you agree with this nurse, to ...   \n",
       "8  * QUESTION * Do you agree with this nurse, to ...   \n",
       "9  * QUESTION * Do you agree with this nurse, to ...   \n",
       "\n",
       "                                            comments  \n",
       "0  \\n                    Did everyone just sudden...  \n",
       "1  \\n                    /pol/ : do you agree wit...  \n",
       "2   why does she dislike the white areas of Georg...  \n",
       "3   As a Nurse, shouldn't she be trying to provid...  \n",
       "4   Summa Cum Laude and she still can't speak pro...  \n",
       "5    makes ya think why they don't and only do it...  \n",
       "6    Shes a nigger psycho, she has a mindless imp...  \n",
       "7    It can’t even form a complete sentence. It c...  \n",
       "8                                it's latin, retard   \n",
       "9    lol hate to brake it to ya honey but sav and...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alle[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentences with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nurse_sample = nlp(alle ['comments'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nurse_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SPACE   \n",
      "As SCONJ prep as\n",
      "a DET det a\n",
      "Nurse PROPN pobj Nurse\n",
      ", PUNCT punct ,\n",
      "should VERB aux should\n",
      "n't PART neg not\n",
      "she PRON nsubj -PRON-\n",
      "be AUX aux be\n",
      "trying VERB ROOT try\n",
      "to PART aux to\n",
      "provide VERB xcomp provide\n",
      "medical ADJ amod medical\n",
      "care NOUN dobj care\n",
      "and CCONJ cc and\n",
      "prevent VERB conj prevent\n",
      "physical ADJ amod physical\n",
      "harm NOUN dobj harm\n",
      "to ADP dative to\n",
      "folks NOUN pobj folk\n",
      "... PUNCT punct ...\n",
      "not PART neg not\n",
      "to PART aux to\n",
      "be AUX aux be\n",
      "encouraging VERB xcomp encourage\n",
      "it PRON dobj -PRON-\n",
      "? PUNCT punct ?\n",
      "? PUNCT punct ?\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used the first half of the scrape because the Matcher only processes 1000000 strings and nurse_text has 1444217.\n",
    "length = len(nurse_text)\n",
    "middle_index = length//2\n",
    "first_half = nurse_text[:middle_index]\n",
    "second_half = nurse_text[middle_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(str(first_half))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(str(second_half))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9880432912405939014, 728, 729), (9880432912405939014, 1404, 1405), (9880432912405939014, 1565, 1566), (9880432912405939014, 2308, 2309), (9880432912405939014, 2521, 2522), (9880432912405939014, 4587, 4588), (9880432912405939014, 6222, 6223), (9880432912405939014, 9101, 9102), (9880432912405939014, 10613, 10614), (9880432912405939014, 12211, 12212), (9880432912405939014, 12277, 12278), (9880432912405939014, 12858, 12859), (9880432912405939014, 13079, 13080), (9880432912405939014, 13142, 13143), (9880432912405939014, 14567, 14568), (9880432912405939014, 14895, 14896), (9880432912405939014, 19351, 19352), (9880432912405939014, 20591, 20592), (9880432912405939014, 21683, 21684), (9880432912405939014, 24283, 24284), (9880432912405939014, 32527, 32528), (9880432912405939014, 33829, 33830), (9880432912405939014, 37466, 37467), (9880432912405939014, 40983, 40984), (9880432912405939014, 41534, 41535), (9880432912405939014, 44914, 44915), (9880432912405939014, 45183, 45184), (9880432912405939014, 49795, 49796), (9880432912405939014, 53979, 53980), (9880432912405939014, 54145, 54146), (9880432912405939014, 54249, 54250), (9880432912405939014, 54506, 54507), (9880432912405939014, 54856, 54857), (9880432912405939014, 55009, 55010), (9880432912405939014, 55145, 55146), (9880432912405939014, 59084, 59085), (9880432912405939014, 60548, 60549), (9880432912405939014, 62568, 62569), (9880432912405939014, 63142, 63143), (9880432912405939014, 63398, 63399), (9880432912405939014, 64106, 64107), (9880432912405939014, 64126, 64127), (9880432912405939014, 64135, 64136), (9880432912405939014, 64456, 64457), (9880432912405939014, 65112, 65113), (9880432912405939014, 65119, 65120), (9880432912405939014, 66710, 66711), (9880432912405939014, 66739, 66740), (9880432912405939014, 66838, 66839), (9880432912405939014, 66950, 66951), (9880432912405939014, 66982, 66983), (9880432912405939014, 67022, 67023), (9880432912405939014, 67635, 67636), (9880432912405939014, 69742, 69743), (9880432912405939014, 70295, 70296), (9880432912405939014, 70776, 70777), (9880432912405939014, 72422, 72423), (9880432912405939014, 72444, 72445), (9880432912405939014, 72489, 72490), (9880432912405939014, 72533, 72534), (9880432912405939014, 73455, 73456), (9880432912405939014, 73461, 73462), (9880432912405939014, 73497, 73498), (9880432912405939014, 73503, 73504), (9880432912405939014, 73989, 73990), (9880432912405939014, 74011, 74012), (9880432912405939014, 74807, 74808), (9880432912405939014, 77463, 77464), (9880432912405939014, 82999, 83000), (9880432912405939014, 87795, 87796), (9880432912405939014, 90828, 90829), (9880432912405939014, 91350, 91351), (9880432912405939014, 92878, 92879), (9880432912405939014, 93269, 93270), (9880432912405939014, 95771, 95772), (9880432912405939014, 97651, 97652), (9880432912405939014, 105401, 105402), (9880432912405939014, 107320, 107321), (9880432912405939014, 107579, 107580), (9880432912405939014, 107664, 107665), (9880432912405939014, 108126, 108127), (9880432912405939014, 110142, 110143)]\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "phrase_list = ['bitch', 'hero']\n",
    "phrase_patterns = [nlp(text) for text in phrase_list]\n",
    "matcher.add(\"nurse_pattern\", None, *phrase_patterns)\n",
    "found_matches = matcher(doc3)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9880432912405939014, 813, 814), (9880432912405939014, 1812, 1813), (9880432912405939014, 3403, 3404), (9880432912405939014, 4073, 4074), (9880432912405939014, 4300, 4301), (9880432912405939014, 5056, 5057), (9880432912405939014, 5077, 5078), (9880432912405939014, 12570, 12571), (9880432912405939014, 21436, 21437), (9880432912405939014, 21619, 21620), (9880432912405939014, 24145, 24146), (9880432912405939014, 35454, 35455), (9880432912405939014, 36163, 36164), (9880432912405939014, 37426, 37427), (9880432912405939014, 38558, 38559), (9880432912405939014, 39471, 39472), (9880432912405939014, 40187, 40188), (9880432912405939014, 41986, 41987), (9880432912405939014, 43000, 43001), (9880432912405939014, 43238, 43239), (9880432912405939014, 43298, 43299), (9880432912405939014, 43313, 43314), (9880432912405939014, 45469, 45470), (9880432912405939014, 45497, 45498), (9880432912405939014, 45522, 45523), (9880432912405939014, 46187, 46188), (9880432912405939014, 46446, 46447), (9880432912405939014, 47974, 47975), (9880432912405939014, 48078, 48079), (9880432912405939014, 54785, 54786), (9880432912405939014, 57272, 57273), (9880432912405939014, 62999, 63000), (9880432912405939014, 84171, 84172), (9880432912405939014, 87070, 87071), (9880432912405939014, 87420, 87421), (9880432912405939014, 88254, 88255), (9880432912405939014, 88965, 88966), (9880432912405939014, 91678, 91679), (9880432912405939014, 91702, 91703), (9880432912405939014, 97063, 97064), (9880432912405939014, 100122, 100123), (9880432912405939014, 100690, 100691), (9880432912405939014, 105391, 105392), (9880432912405939014, 105622, 105623), (9880432912405939014, 108781, 108782), (9880432912405939014, 109922, 109923), (9880432912405939014, 110741, 110742), (9880432912405939014, 111734, 111735), (9880432912405939014, 112258, 112259), (9880432912405939014, 116983, 116984), (9880432912405939014, 117559, 117560), (9880432912405939014, 119000, 119001), (9880432912405939014, 119381, 119382), (9880432912405939014, 120515, 120516), (9880432912405939014, 121712, 121713), (9880432912405939014, 123609, 123610), (9880432912405939014, 125717, 125718), (9880432912405939014, 130110, 130111), (9880432912405939014, 130220, 130221), (9880432912405939014, 130403, 130404), (9880432912405939014, 134513, 134514), (9880432912405939014, 136275, 136276), (9880432912405939014, 141069, 141070), (9880432912405939014, 142695, 142696), (9880432912405939014, 143309, 143310), (9880432912405939014, 146752, 146753), (9880432912405939014, 148229, 148230), (9880432912405939014, 158928, 158929), (9880432912405939014, 167117, 167118), (9880432912405939014, 171568, 171569), (9880432912405939014, 171585, 171586), (9880432912405939014, 185267, 185268)]\n"
     ]
    }
   ],
   "source": [
    "# second half of the scrape\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "phrase_list = ['bitch', 'hero']\n",
    "phrase_patterns = [nlp(text) for text in phrase_list]\n",
    "matcher.add(\"nurse_pattern\", None, *phrase_patterns)\n",
    "found_matches = matcher(doc4)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matches, first half of scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LEMMA\":\"hero\"}]\n",
    "matcher.add(\"nurse_pattern\", None, pattern)\n",
    "matches = matcher(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches = matcher(doc3)\n",
    "# found_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [sent for sent in doc3.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*clap* heroes!\n"
     ]
    }
   ],
   "source": [
    "# Show the 1st sentence, which contains a match\n",
    "for exempli in sents:\n",
    "    if found_matches[0][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a hero. ', ' ', ' >\n"
     ]
    }
   ],
   "source": [
    "# Show the 2nd sentence, which contains a match\n",
    "for exempli in sents:\n",
    "    if found_matches[1][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It makes it worse that these young American women are supposedly hero’s and have a god complex.\n"
     ]
    }
   ],
   "source": [
    "# Show the 3rd sentence, which contains a match\n",
    "for exempli in sents:\n",
    "    if found_matches[2][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257898901Most nurses dont give 2 fucks about their job, they became nurses to meet and marry a doctor. ', ' https://www.strawpoll.me/20087442>>257898901>>257899031>>257899169>>257899428>>257899428>>257899501>>257899544 ', ' >>257898901Anyone seen those new yard signs that say \"A hero lives here\"?Can\n"
     ]
    }
   ],
   "source": [
    "for exempli in sents:\n",
    "    if found_matches[3][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is not a factual statement on which group are heros.\n"
     ]
    }
   ],
   "source": [
    "for exempli in sents:\n",
    "    if found_matches[97][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break\n",
    "# 97 seems to be the last \"hero\" in first half of scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matches, second half of scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LEMMA\":\"hero\"}]\n",
    "matcher.add(\"nurse_pattern\", None, pattern)\n",
    "matches = matcher(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches2 = matcher(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents2 = [sent for sent in doc4.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A lot of people are wondering if they'll be able to feed their kids in the next few months because of over-reacting medical 'professionals' in the media blowing things out of proportion. \", ' If a few months of actual hard work makes you feel like \"a hero without a cape\" or \"an underappreciated protector\" then your profession isn\\'t used to hard workMost thots became nurses for social validation and so that they could appear virtuous Meanwhile cops, doctors, industrial engineers, carpenters and any other actually important and\n"
     ]
    }
   ],
   "source": [
    "for exempli in sents2:\n",
    "    if found_matches2[0][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", ' The real heroes ', '\n"
     ]
    }
   ],
   "source": [
    "for exempli in sents2:\n",
    "    if found_matches2[5][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "military folks get praise because they are mostly men and most men dont give two shits about being called a hero.\n"
     ]
    }
   ],
   "source": [
    "for exempli in sents2:\n",
    "    if found_matches2[21][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break\n",
    "# 21 seems to be last \"hero\" in second half of scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LEMMA\":\"bitch\"}]\n",
    "matcher.add(\"nurse_pattern\", None, pattern)\n",
    "matches = matcher(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches3 = matcher(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents3 = [sent for sent in doc4.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumb bitch.\n"
     ]
    }
   ],
   "source": [
    "for exempli in sents3:\n",
    "    if found_matches3[3][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", ' bump ', ' >>240879713karma is a fantastic bitch get rekt dog boiling chinkcels ', \" >>240880585\n"
     ]
    }
   ],
   "source": [
    "for exempli in sents3:\n",
    "    if found_matches3[83][1] < exempli.end:\n",
    "        print(exempli)\n",
    "        break\n",
    "# 83 seems to be the last \"bitch\" in second half of scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
